<h5 align="center"><em>I'm currently updating this site, but in the meantime a quick snapshop about me...</em></h5>

<h1 align="center">Hi üëã, I'm Albert</h1>

<h3 align="center">A business analytics professional & full stack R developer, former Big-4 consultant and in-house financial analyst with over 10 years‚Äô experience, passionate about fostering the data analytics practice across organizations & teams to drive business insights. Holder of a bachelor degree in Economics, an MSc in Data Analytics and several technical diplomas.</h3>

<p align="center">
<a href="https://www.linkedin.com/in/albertcampillo/" target="blank"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/linked-in-alt.svg" alt="https://www.linkedin.com/in/albertcampillo/" height="25" width="35" /></a>
<a href="https://twitter.com/albertcampillo" target="blank"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/twitter.svg" alt="albertcampillo" height="25" width="35" /></a>
<a href="https://www.kaggle.com/albertcampillo" target="blank"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/kaggle.svg" alt="https://www.kaggle.com/albertcampillo" height="25" width="35" /></a>
</p>

<h3 align="left">More about me:</h3>
      
- üìç I was born & raised in **Spain**, studied in **Canada** & **US**, and lived in **Switzerland** for the past 10 years.

- üì£ I can speak **English**, **French**, **Spanish** & **Catalan** fluently; conversational **Italian** & I recently started learning **Korean**.

- üå± I‚Äôm currently **preparing** to take the 3 Microsoft Azure essentials certifications (Azure Essentials, Azure AI Essentials & Azure Data Essentials) end of April'22

- üìà I'm **completing** the course **High Performance Time Series in R** by Business Science University... a game changer for time series analysis.

- üöÄ I am **scaling** my knowledge of **Shiny** (an open source technology to delevop user apps & dashboards) to meet the standards of my previous deliverables in **Tableau** & **Power BI** 


<details><summary><strong>Languages and Tools</strong> I use</summary>
  <blockquote>
    <br>
    <a href="https://www.rstudio.com/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/rstudio/rstudio-original.svg" alt="rstudio" width="35" height="35"/></a>
    <a href="https://www.shinyapps.io/" target="_blank" rel="noreferrer"><img src="https://github.com/acampi/acampi/blob/main/shiny.png" alt="shiny" width="35" height="35"/></a> 
  <a href="https://h2o.ai/" target="_blank" rel="noreferrer"><img src="https://github.com/acampi/acampi/blob/main/h2o.jpg" alt="h2o" width="35" height="35"/></a>
  <a href="https://www.tidyverse.org/" target="_blank" rel="noreferrer"><img src="https://github.com/acampi/acampi/blob/main/tidyverse.png" alt="tidyverse" width="35" height="35"/></a> 
  <a href="https://www.tidymodels.org/" target="_blank" rel="noreferrer"><img src="https://github.com/acampi/acampi/blob/main/tidymodels.png" alt="tidymodels" width="35" height="35"/></a>
  <a href="https://business-science.github.io/timetk/" target="_blank" rel="noreferrer"><img src="https://github.com/acampi/acampi/blob/main/timetk.png" alt="timetk" width="35" height="35"/></a>
  <a href="https://business-science.github.io/modeltime/" target="_blank" rel="noreferrer"> <img src="https://github.com/acampi/acampi/blob/main/modeltime.png" alt="modeltime" width="35" height="35"/></a>
  <a href="https://www.python.org" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" alt="python" width="35" height="35"/></a>
  <br>
  <a href="https://www.w3.org/html/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/html5/html5-original-wordmark.svg" alt="html5" width="35" height="35"/></a>
  <a href="https://www.w3schools.com/css/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/css3/css3-original-wordmark.svg" alt="css3" width="35" height="35"/></a>
  <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/javascript/javascript-original.svg" alt="javascript" width="35" height="35"/></a>
  <br>
  <a href="https://www.docker.com/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/docker/docker-original-wordmark.svg" alt="docker" width="35" height="35"/></a>
  <a href="https://cloud.google.com" target="_blank" rel="noreferrer"><img src="https://www.vectorlogo.zone/logos/google_cloud/google_cloud-icon.svg" alt="gcp" width="35" height="35"/></a> 
  <a href="https://git-scm.com/" target="_blank" rel="noreferrer"><img src="https://www.vectorlogo.zone/logos/git-scm/git-scm-icon.svg" alt="git" width="35" height="35"/></a>  
  <a href="https://www.mongodb.com/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/mongodb/mongodb-original-wordmark.svg" alt="mongodb" width="35" height="35"/></a>
  <a href="https://www.mysql.com/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/mysql/mysql-original-wordmark.svg" alt="mysql" width="35" height="35"/></a>  
  <a href="https://www.postgresql.org" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/postgresql/postgresql-original-wordmark.svg" alt="postgresql" width="35" height="35"/></a>  
  <a href="https://reactjs.org/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/react/react-original-wordmark.svg" alt="react" width="35" height="35"/></a>
  <a href="https://www.photoshop.com/en" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/photoshop/photoshop-line.svg" alt="photoshop" width="35" height="35"/></a>
  <br>
  </blockquote>
</details>

<hr>

<h3 align="left">My Portfolio</h3>
Welcome to my portfolio! In this section you will find a glimpse of the current projects I have been working on recently. Projects are grouped into 2 different categories:
<br>
<ul>
  <li><strong>Shiny Apps</strong>: a series of data science-related projects, some of which are inspired in previous deliveries of my past working experience. Others are topics from my own areas of interest. Each project comprises of a business problem to solve, the approach to solution (rationale) and the methodology applied. The end result is presented as a simple, shiny app.</li>
  Apps are categorized in three groups depending on each of the following icons:
    <ul>
      <li>‚úÖ: app is live and can be checked at any time on the app link </li>
      <li>üî∑: app is not live. If interested, please reach out and I will grant temporary access</li>
      <li>üî∂: work in progress. The project/ app will be completed soon</li>
    </ul> 
  <br>
  <li><strong>Data Visualizations</strong>: A selection of data visualizations I created from open source datasets, leveraging the power of data visualization packages in r such as <em>ggplot2</em>, <em>plotly</em>, <em>gt</em>, <em>gtExtra</em> & <em>reactable</em>.</li>
</ul>
Kindly use the ‚ñ∂Ô∏è buttons to navigate through the expandable/collapse sections/ projects/ details.
<br><br>
<details open><summary><strong> üíª Shiny Apps </strong></summary>
  <blockquote>
  <br>
  
  <!-- Project 1 -->
  <details><summary>‚úÖ1. <strong>Nested Time Series:</strong> Forecasting Demand for a Company's Portfolio Products <a href="https://campillo.shinyapps.io/customer_lifetime_value_app/">app</a></summary>
    <blockquote>
      <br>
      <p>A well-known company in the retail industry wants to better understand the future demand for their products in views of optimizing their internal supply chain. Their product portfolio comprises 100 different SKUs and they seek a solution to forecast demand for multiple products at scale. The dataset comes from <em>Kaggle's M5 competition</em> and can be found <a href="https://www.kaggle.com/c/m5-forecasting-accuracy"> here</a></p>
      <ul>
        <li><strong>Business Problem: How can we forecast future demand for many products at once? </strong>In other words, how can we automate time series forecasting on big scale?:</li>
        <br>
        <li><strong>Rationale</strong>: the approach used solves demand forecasting for 1 time series (1 product) first and, once validated, it is scaled to all other products through an <em>iterative</em> process. This process is executed with three key considerations in mind:</li> 
        <ul>
          <li>1. Demand forecast considers <strong> 90 days forward</strong> (future period) for each product </li>
          <li>2. Each time series will run into <strong>three different ML models</strong> (2 XGBoost & 1 Temporal Hierarchical Forecasting <em>explained below</em>, each one producing a forecast. The best performing model within the test set will be selected and stored to forecast demand for the future period </li>
          <li>3. The iterative process uses a <strong>Nested Modelling</strong> approach (time consuming but results accurate)</li>
        </ul>
        <br>
        <li><strong>Final Result:</strong></li>
        <img src="https://github.com/acampi/acampi/blob/main/nested_ts_3.png" alt="nested timeseries"/>
        <br>
        <li><strong>Methodology:</strong></li>
          <details open><summary> see in-depth procedure </summary>
          <blockquote>
          <img src="https://github.com/acampi/acampi/blob/main/nested_ts_1.png" alt="methodology"/>
          <br>
          This is the step-by-step methodology implemented & described in previous section. In a nutshell, for each time series, we apply 3 ML models (each one producing a forecast). Results are compared against a test set (actual data). The best performing model is stored via an iterative process (nested modeling) and later used to retrain on the full dataset prior to forecast 90 days forward (future period).
          <br><br>
          <ul>
            <li><strong>1. Data Preparation</strong>. The dataset is treated as a nested structure, where each product is handled individually. Steps: </li> 
            <ul>
              <li>Group dataset by product (time series) </li>
              <li>Split each time series into <strong>train set</strong> (80% of data) and <strong>test set</strong> (remaining 20%). Assumption: test set equals last 90 days of actual data)</strong> </li>
              <li>Extend actual timeseries 90 days forward into the future (at this stage, it is an empty 90 days dataset with empty values)</li>
            </ul>
            <br>
            <li><strong>2. Data Preprocessing for ML</strong></li>
            Prepare the dataset (equivalent to preparing a <strong>recipe</strong>) to be used with each ML model accordingly. Therefore:     
            <br>
            <ul>
              <li><strong>XGBoost recipe</strong>: demand as a function of all predictors.</li> 
                  A bunch of time series related predictors have been generated from date variable (date removed from modeling, along with zero variance predictors). Some of these predictors are later converted into dummy variables.  
                  <li><strong>THIEF</strong> (Temporal Hierarchical Forecasting): an algorithm that aggregates the time series and ensembles the forecast. For this one, we will just use the preliminary dataset (no additional features created in a recipe)</li>
            </ul>
            <br>
            <li><strong>3. Model Preparation</strong>: define the <strong>workflow</strong> to be applied to each of the 3 models deployed.</li>
            Each workflow consists of a model definition (either xgboost or thief in our case) + the recipe prepared for each. As mentioned earlier, 2 XGBoost models are used. The difference between them is the learn rate applied (0.35 for Model 1, 0.50 for Model 2)            
            <img src="https://github.com/acampi/acampi/blob/main/nested_ts_2_1.png" alt="methodology"/>
            <br>
            <li><strong>4. Try 1 Time Series First, then the next 99 </strong></li>
            Take 1 Time series, fit each of the 3 models into the train & test sets. Once this first process is completed and verified with no errors, the remaining 99 time series will run iteratively
          </ul>
          </blockquote>
          </detail>
      </ul>
    </blockquote>
  </details>

  
  <!-- Project 2-->
  <details><summary>‚úÖ2. Predicting <strong>Customer Lifetime Value</strong> (CLV) <a href="https://campillo.shinyapps.io/customer_lifetime_value_app/">app</a></summary>
    <blockquote>
      <br>
      <p>A company called CDNow is interested to determine the <strong>Customer Lifetime Value (CLV)</strong> of their customer base. The CLV is the estimated profit from the future relationship with a given customer. To do so, I use the company's sales records, a transactional database of 65k sales from 23k customers during the period Jan'97 to Jun'98 (18 months)</p>
      <ul>
        <li><strong>Business Problem: Which customers should a company focus on?</strong> Focus on those with the <em>greatest future spend</em> and <em>highest probability</em> of future spend. Once the prioritization problem is addressed, CDNow will be able to make informed decisions on questions such as:</li>
        <ul>
          <li><em>Which customers have the highest spend probability in the next N-days?</em></li>
          <li><em>Which customers recently purchased but are unlikely to buy again?</em></li>
          <li><em>Which customers were predicted to purchase but didn't (missed opportunities)</em></li>
        </ul>
        <br>
        <li><strong>Rationale:</strong> Define the CLV for each customer. 
        Assumptions:
        <ul>
          <li>Litefime Value will be determined within the next 90 days time horizon </li>
          <li>CLV based on a 2-side approach and will use 2 machine learning models:</li>
          <ol>
            <li>How much will a customer spend in the next 90 days? <em>Regression Model</em></li>
            <li>What is the probability that a customer will make another purchase in the next 90-days?  <light><em>Classification Model</em></light></li>
          </ol>
        </ul>
        <br>
        <li><strong>Final Result:</strong></li>
        <img src="https://github.com/acampi/acampi/blob/main/clv_5.png" alt="shiny"/>
        <br>
        <br>
        <li><strong>Methodology:</strong></li>
          <details open><summary> see in-depth procedure </summary>
          <blockquote>
          <img src="https://github.com/acampi/acampi/blob/main/clv_1.png" alt="methodology"/>
          bla bla bla
          <ul>
            <li><strong>1. Cohort Definition</strong> <em> (my cohort: first time purchasers within 90-days time window)</em></li>
              <ul>
                <li>Find min date for each customer</li>
                <li>Check span of dates across dataset <em>(i.e. Jan‚Äô97 - Jun‚Äô98)</em> and select customers within the first 90 days range <em>(ie. Jan‚Äô97 - Mar‚Äô97)</em> </li>
                <li>Subset dataset showcasing transactions for those customers only</li>
                <br>
                <img src="https://github.com/acampi/acampi/blob/main/clv_2.png">
              </ul>
            <br>
            <li><strong>2. Data Preprocessing for ML</strong></li>
            <ul>
              <li><strong>Random customer splitting:</strong> subset the full database into</li>
              <ul>
                <li><strong>Split 1</strong> (<em> for model training</em>): 80% of customers' transactions (train)</li>
                <li><strong>Split 2</strong> (<em>for model accuracy</em>): remaining 20% of customers' transactions (test)</li>
              </ul>
              <li><strong>Time splitting</strong>: For both Split 1 & Split 2, break down the dataset into 2 time sets</li>
              <ul>
                <li><strong>Train Set</strong> = Full Set - 90 days </li>
                <li><strong>Test Set</strong> = Last 90 days </li>
              </ul>
            </ul>
            <br>
            <li><strong>3. Feature Engineering</strong></li>
            Define the target variables and the predictors to use in the ML models 
            <ul>
              <li><strong>Target variables</strong>:</li>
              <ul>
                <li><strong>spend_90_total</strong>: total amount spend in the next 90 days</li>
                <li><strong>spend_90_prob</strong>: probability of spend in the next 90 days</li>
              </ul>
              <br>
              <li><strong>Predictors</strong>(RFM Features):</li>
              <ul>
                <li><strong>Recency:</strong> how many days since the last purchase</li>
                <li><strong>Frequency:</strong> number of purchases a customer has done during the train period</li>
                <li><strong>Monetary:</strong> total purchase value of a given customer during train period</li>
                <li><strong>Monetary mean:</strong> mean purchase value of a given customer among all purchases during train period</li>
              </ul>
            </ul>
            <img src="https://github.com/acampi/acampi/blob/main/clv_3.png" alt="methodology"/>
            <br>
            <li><strong>4. ML Model Phase </strong>: </li>
            I preliminarly define the target variable and set of predictors upon which each ML model will be fit into the train set. The 2 models used are:  
            <ul>
              <li><strong>Regression model</strong>: predict amount spent of a given customer in the next 90 days. I use XGBoost algorithm for regression and model performance is measured through <strong>RSME</strong></li>
              <li><strong>Classification model</strong>: predict the probability of a given customer purchasing in the next 90 days. I also use XGBoost for classification and model performance is measured through <strong>logloss</strong></li>
            <ul>
          </ul>
          </blockquote>
          </detail>
        <br>
        <br>
      <li><strong>Final Result</strong></li>
      </ul>
    </blockquote>
  </details>
  
    
  <!-- Project 3 -->
  <details><summary>‚úÖ3. NASDAQ Stock Analyzer <a href="https://campillo.shinyapps.io/02_stock_screener_app/">app</a> using <strong> Apache Spark </strong></summary>
    <blockquote>
      <br>
      <p>A retail investor is interested in growing his/ her personal wealth by building a portfolio of stocks traded at NASDAQ. This individual is not a savvy investor but seeks to understand which stocks' have had a good past performance to further assess whether to pick them or not. The dataset used comprises daily stock performance for over 4,500 stocks during the last 10 years (5.8 million instances).
</p>
      <ul>
        <li><strong>Business Problem</strong>: Hundreds of stocks traded in NASDAQ to sift through. Which ones to pick? </li>
        Two preliminary considerations need to be addressed for this problem:
        <ul>
          <li>1. Which metric(s) will be used to differentiate between good and bad stocks?</li>
          <li>2. How to deal with a massive dataset efficiently?</li>
        </ul>
        <br>
        <li><strong>Rationale:</strong> :</li>
        <ol>
          <li><strong> Business consideration</strong>. For each stock, I will use these <em>two metrics</em>combined:</li>
            <ul>
              <li><strong>Profitability metric </strong>: daily average return (how much the stock increases per day, in percentual terms)</li>
              <li><strong>Risk metric</strong>: standard deviation of its return (measure of a stock's volatility) </li>
            </ul>
          <li><strong>Tehcnical consideration.</strong> This large dataset needs a big data approach.</li>
        </ol>
        <br>
        <li><strong>Final Result:</strong></li>
        <img src="https://github.com/acampi/acampi/blob/main/spark_2.png" alt="shiny"/>
        <br>
        <li><strong>Methodology:</strong></li>
          <details open><summary> see in-depth procedure (coded)</summary>
          <blockquote>
          <img src="https://github.com/acampi/acampi/blob/main/spark_1.png" alt="methodology"/>
          <br>
          <ul>
            <li><strong>Part 1: Spark</strong></li>
            For simplicity purposes, I will ran this part locally (Java installation required), but ideally I would ran it in the cloud via a cloud service provider (i.e. Databricks, AWS...)
            <ul>
              <li><strong>Connect</strong> to spark & set up main configurations: 
                    i) number of cores to run to, 
                    ii)laptop RAM memory in use, 
                    iii) what fraction of previously defined RAM will be used for the process</li>
              <li><strong>Data Wrangling</strong>. The dataset is then wrangled in a big data cluester, where I calculate the main performance metrics. The end result is a reduced dataset that will be used in R to do the main data analysis & end visualizations.</li>
                <ul>
                  <li>Load the dataset, grouped by stock ticker, arranged by date</li>
                  <li>Calculate daily returns & summarise the given stock by the mean returns and standard deviation of its returns</li>
                </ul>
            </ul>
            <br>
            <li><strong>Part 2: Data Wrangling/ Analysis in R</strong></li>
            <ul>
              <li>To simplify the search I will focus on stocks with market cap avove $1bio & standad deviation below 1</li>
              <li>The reward metric is based on a variation of the Sharpe ratio (mean return over standard deviation). The greater the ratio, the better the stock is.</li>
            </ul>
            <br>
          </ul>
          </blockquote>
          </detail>
        <br>
        <br>
       </ul>
    </blockquote>
  </details>
          
          
  <!-- Project 6 -->
  <details><summary>üî∂4. <strong>Forecasting </strong> Singapore's Resale Housing Market</summary>
    <blockquote>
      <br>    
      More on this project coming soon
    </blockquote>
  </details>
    
  </blockquote>
</details>



<br>
<details open><summary><strong> üìä Data Visualizations </strong></summary>
<blockquote>
   <br>
   <details><summary><strong> My Visualizations </strong></summary>
      <blockquote>
        A collection of data visualizations that I created from public data sources. 
        <a href="https://github.com/acampi/DataVisualizations">Go to repository</a>
      </blockquote>
   </details>
   <details><summary><strong> Tidy Tuesday </strong></summary>
      <blockquote>
            My contributions to #TidyTuesday challenge, a weekly social data project by Thomas Mock and R4DS Online Learning Community that focuses on data wrangling and visualisation. <a href="https://github.com/acampi/Tidytuesday">Go to repository</a>
      </blockquote>
   </details>
  </blockquote>
</details>


<hr>



<h3 align="left">My learning journey in Analytics</h3>

<details><summary><em> explore full track</em> üëÄ</summary><blockquote>
<a href="https://www.rstudio.com/" target="_blank" rel="noreferrer"> <img src="https://github.com/acampi/acampi/blob/main/learning_path.png" alt="rstudio"/> </a>

<details><summary> 1. The <strong>Master of Science in Business Analytics</u></strong> program:</summary><blockquote>
  <p> ‚ö†Ô∏èSection under construction‚ö†Ô∏è</p> 
<p> In the meantime, you can find more about <strong>NYU MSBA program</strong><a href="https://www.stern.nyu.edu/programs-admissions/ms-business-analytics/academics"> here </a></p>
  </blockquote></details>
  
<details><summary> 2.The <strong>Full Stack Web Development</u></strong> bootcamp:</summary><blockquote>
  <p> ‚ö†Ô∏èSection under construction‚ö†Ô∏è </p>
  <p> In the meantime, you can find more about <strong>Le Wagon program</strong><a href="https://www.lewagon.com/web-development-course/full-time"> here </a></p>
  </blockquote></details>

<details open><summary> 3. The <strong><u>Full Stack R Developer</u></strong> 5 course-track:</summary><blockquote>
<p><a href="https://www.business-science.io/">Business Science University</a> is an online learning platform created by Matt Dancho with a state-of-the-art, hands-on and practical business oriented methodology to learn R & Python. </p>
  <p>The <strong> Full Stack R Developer</strong> track is a 6-month course to become a full stack R developer, capable of <strong>deploying machine learning solutions in high-performing, scalable web apps in the cloud.</strong></p>
  <a href="https://www.rstudio.com/" target="_blank" rel="noreferrer"> <img src="https://github.com/acampi/acampi/blob/main/course_track.png" alt="rstudio"/> </a>
  <p>An indepth syllabus of the entire track can be checked as follows:</p>
  
<!-- Course 1-->
<details><summary> ‚úÖ <strong>1. Business Analysis with R </strong>üìä</summary><blockquote>
  <p> Foundational data science & manipulation course using R & tidyverse, covering: </p>
  <ul>
    <li><strong>Data Import</strong>: readr & odbc</li>
    <li><strong>Data Cleaning</strong> & <strong>Wrangling</strong>: dplyr & tidyr</li>
    <li><strong>Time Series</strong>, <strong>Text</strong>, & <strong>Categorical Data</strong>: lubridate, stringr, & forcats</li>
    <li><strong>Visualization</strong>: ggplot2</li>
    <li><strong>Functional programming</strong> & <strong>Iteration</strong>: purrr</li>
    <li><strong>Modeling</strong> & <strong>Machine Learning</strong>: parnsip (xgboost, glmnet, kernlab, broom, & more)</li>
    <li><strong>Business Reporting</strong>: communicate results with rmarkdown</li>
  </ul>
  <a href="" target="_blank" rel="noreferrer"> <img src="https://github.com/acampi/acampi/blob/main/certif_ds4b_101R.png" alt="ds4b_101R" width="400" height="300"/></a>
</blockquote></details>

<!-- Course 2-->
<details><summary> ‚úÖ <strong>2. Data Science for Business with R</strong> ü§ñüîÆ</summary><blockquote>
<br>
  <p>Practical data science course encompassing the use of <a href="https://h2o.ai/">H2O AutoML</a> open-source machine learning framework to solve business problems.</p>
  <ul>
    <li>Business problem <strong>foundations</strong>, introduction to the <strong>Business Problem Framework</strong></li>
    <li><strong>Business Understanding</strong>: Using dplyr & ggplot2 to size the business problem tidy eval to build custom functions that fit within the tidyverse</li>
    <li><strong>Data Understanding</strong>: Use skimr and GGally packages to efficiently visualize key relationships</li>
    <li><strong>Data Preparation</strong>: Use recipes to prepare data in both human and machine readable formats | perform preliminary correlation analysis</li>
    <li><strong>H2O AutoML Modeling</strong> & <strong>Performance</strong>: Use Automated Machine Learning (AutoML) to produce 30+ models | analyze performance using ROC, Precision/Recall, Gain & Lift plots</li>
    <li>Explaining <strong>Black-Box Modela</strong>: Use LIME to explain which features are driving the complex deep learning & stacked ensemble models</li>
    <li><strong>Expected Value</strong>, <strong>Threshold Optimization</strong>, & <strong>Sensitivity Analysis</strong>: Link the model to financial performance through the Expected Value framework</li>
    <li><strong>Recommendation Algorithm Development</strong>: Use a 3-step process to develop a recommendation algorithm capable of assisting managers in retaining employees</li>
  </ul>
  <a href="" target="_blank" rel="noreferrer"> <img src="https://github.com/acampi/acampi/blob/main/certif_ds4b_201R.png " alt="ds4b_201R" width="400" height="300"/></a>
</blockquote></details>

<!-- Course 3-->
<details><summary> ‚öôÔ∏è <strong>3. High Performance Time Series Forecasting</strong> üïìüìàüìâ </summary><blockquote>
<br>
<p>Apply the latest forecasting techniques to real business problems by learning from the strategies 
  that won 4 key time-series Kaggle competitions</p>
<ul>
  <li>Improve <strong>demand forecasting</strong></li>
  <li><strong>Advanced forecasting</strong> algorithms & <strong>feature engineering</strong></li>
  <li><strong>Time Series preparation</strong> `time_tk`: time series data wrangling, transformations & visualization</li>
  <li>Machine learning `modeltime`: <strong>time series modeling</strong>, <strong>experimentation</strong> & <strong>model comparison</strong></li>
  <li><strong>Deep Learning</strong> with `gluon`</li>
</ul> 
</blockquote></details>

<!-- Course 4-->
<details><summary> ‚úÖ <strong>4. Shiny Web Applications</strong> üîß‚öôÔ∏è</summary><blockquote>
<br>
  <p>Deployment of data solutions into web applications using the Shiny & Flexdashboard frameworks</p>
  <ul>
    <li><strong>Shiny</strong>: A web application framework with UI components that are reactive to user input.</li>
    <li><strong>Flexdashboard</strong>: A dashboarding framework that is built on top of RMarkdown.</li>
    <li><strong>Machine learning models</strong> used to predict product prices: parsnip and XGBoost</li>
  </ul>
  <a href="" target="_blank" rel="noreferrer"> <img src="https://github.com/acampi/acampi/blob/main/certif_ds4b_102R.png" alt="ds4b_102R" width="400" height="300"/></a>
</blockquote></details>

<!-- Course 5-->
<details><summary> ‚öôÔ∏è <strong>5. Advanced Shiny Apps with AWS</strong> üõ†Ô∏è‚òÅÔ∏è </summary><blockquote>
<br>
  <p>Build & deploy <strong>complex production-ready apps in the cloud</strong> using R, Shiny & AWS</p>
  <ul>
    <li>Frontend: Shiny integration into <strong>Bootstrap system</strong> & <strong>Shiny Javascript</strong></li>
    <li><strong>Backend development</strong>: authentication, user management for customized & secure UI. Store user work and connect the app to a NoSQL cloud database</li>
    <li><strong>Product deployment</strong> with <strong>AWS</strong> & MongoDB</li>
  </ul>
</blockquote></details>

</blockquote></details>

</blockquote></details>
